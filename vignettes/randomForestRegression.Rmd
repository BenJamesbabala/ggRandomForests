---
title: 'ggRandomForests: Exploring a Random Forest for Regression'
author: "John Ehrlinger<br/>Assistant Staff<br/>Quantitative Health Sciences<br/>Lerner Research Institute<br/>Cleveland Clinic"
date: "`r today <- Sys.Date();format(today, format='%B %d %Y')`"
output: 
  rmarkdown::html_vignette
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{ggRandomForests}
  \usepackage[utf8]{inputenc}
bibliography: ggRandomForests.bib
---

# About this document

This document is a package vignette for the [ggRandomForests](http://CRAN.R-project.org/package=ggRandomForests) package, Visually Exploring Random Forests. 

[ggRandomForests](http://CRAN.R-project.org/package=ggRandomForests)  will help uncover variable associations in the random forests models. The package is designed for use with the [randomForestSRC](http://CRAN.R-project.org/package=randomForestSRC) package for survival, regression and classification forests and uses the [ggplot2](http://CRAN.R-project.org/package=ggplot2) package for plotting diagnostic and association results. [ggRandomForests](http://CRAN.R-project.org/package=ggRandomForests) is  structured to extract data objects from the random forest object and provides S3 functions for printing and plotting these objects.
 
This vignette is written in [markdown](http://daringfireball.net/projects/markdown/), a wiki type language for creating documents. It is support in R using the [rmarkdown](http://rmarkdown.rstudio.com) package, which is especially easy to use within the [RStudio](http://rstudio.com) IDE. A markdown/rmarkdown cheat sheet is available online at (http://rmarkdown.rstudio.com/RMarkdownCheatSheet.pdf). 

The latest version of this vignette is available within the [ggRandomForests](http://cran.r-project.org/web/packages/ggRandomForests/index.html) on [CRAN](http://cran.r-project.org). A development version of the ggRandomForests package is available on [Github](https://github.com) at (https://github.com/ehrlinger/ggRandomForests).

```{r setup, include = FALSE, cache = FALSE, echo = FALSE} 
library(knitr)
# set global chunk options for knitr. These can be changed in the header for each individual R code chunk
opts_chunk$set(fig.path = 'fig-rfr/rfr-', 
              # fig.align = 'center', 
              # size = 'footnotesize', 
               prompt = TRUE, 
               comment = NA, 
               echo = TRUE, results = TRUE, 
               message = FALSE, warning = FALSE, 
               error = FALSE, prompt = TRUE)

# Setup the R environment
options(object.size = Inf, expressions = 100000, memory = Inf, 
        replace.assign = TRUE, width = 90)

#################
# Load_packages #
#################
library(ggplot2) # Graphics engine for generating all types of plots

library(dplyr) # Better data manipulations
library(tidyr)
library(parallel)

library(ggRandomForests)

# Analysis packages.
library(randomForestSRC) 
library(RColorBrewer)

library(xtable)
#library(plot3D)

options(mc.cores = 1, rf.cores = 1)

#########################################################################
# Default computation settings
#########################################################################
theme_set(theme_bw())
```

# Introduction

Random Forests [@Breiman:2001] (RF) are a fully non-parametric statistical method which requires no distributional assumptions on covariate relation to the response. RF is a robust, nonlinear technique that optimizes predictive accuracy by fitting an ensemble of trees to stabilize model estimates. Random Survival Forests [@Ishwaran:2007a; @Ishwaran:2008] (RSF) are an extension of Breiman's RF techniques to survival settings, allowing efficient non-parametric analysis of time to event data. The [randomForestSRC](http://CRAN.R-project.org/package=ggRandomForests) package [@Ishwaran:RFSRC:2014] is a unified treatment of Breiman's random forests for survival, regression and classification problems.

Predictive accuracy make RF an attractive alternative to parametric models, though complexity and interpretability of the forest hinder wider application of the method. We introduce the [ggRandomForests](http://CRAN.R-project.org/package=ggRandomForests) package for visually exploring random forest models. The ggRandomForests package is structured to extract intermediate data objects from randomForestSRC objects and generate figures using the [ggplot2](http://CRAN.R-project.org/package=ggplot2) graphics package [@Wickham:2009].

Many of the figures created by the [ggRandomForests](http://CRAN.R-project.org/package=ggRandomForests) package are also available directly from within the [randomForestSRC](http://CRAN.R-project.org/package=ggRandomForests) package. However, [ggRandomForests](http://CRAN.R-project.org/package=ggRandomForests) offers the following advantages:

* Separation of data and figures: [ggRandomForests](http://CRAN.R-project.org/package=ggRandomForests) contains functions that  operate on either the `randomForestSRC::rfsrc` forest object directly, or on the output from [randomForestSRC](http://CRAN.R-project.org/package=ggRandomForests) post processing functions (i.e. `randomForestSRC::plot.variable`, `randomForestSRC::var.select`, `randomForestSRC::find.interaction`) to generate intermediate [ggRandomForests](http://CRAN.R-project.org/package=ggRandomForests) data objects. S3 functions are provide to further process these objects and plot results using the [ggplot2](http://CRAN.R-project.org/package=ggplot2) graphics package. Alternatively, users can use these data objects for additional custom plotting or analysis operations.  

* Each data object/figure is a single, self contained object. This allows simple modification and manipulation of the data or [ggplot2](http://CRAN.R-project.org/package=ggplot2) objects to meet users specific needs and requirements. 

* The use of [ggplot2](http://CRAN.R-project.org/package=ggplot2) for plotting. We chose to use the [ggplot2](http://CRAN.R-project.org/package=ggplot2) package for our figures to allow users flexibility in modifying the figures to their liking. Each S3 plot function returns either a single [ggplot2](http://CRAN.R-project.org/package=ggplot2) object, or a `list` of [ggplot2](http://CRAN.R-project.org/package=ggplot2) objects, allowing users to use additional [ggplot2](http://CRAN.R-project.org/package=ggplot2) functions or themes to modify and customize the figures to their liking.  

This document is formatted as a tutorial for using the [randomForestSRC](http://CRAN.R-project.org/package=ggRandomForests) for building random forests and post-processing with the [ggRandomForests](http://CRAN.R-project.org/package=ggRandomForests) package for investigating how the forest is constructed. 

In this tutorial, we will focus on the [Boston Housing Data](#data) available in the [MASS](http://CRAN.R-project.org/package=MASS) package. We build a [random forest for regression](#rfr) and demonstrate a full analysis examining that model. 

We investigate the forest [variable selection](#varsel) using the [Variable Importance](#vimp) measure (VIMP) [@Breiman:2001] as well as [Minimal Depth](#minimaldepth) [@Ishwaran:2010], a property derived from the construction of each tree within the forest, to assess the impact of variables on forest prediction. Once we have an idea of which variables the forest is using for prediction, we use [variable dependence](#dependence) plots [@Friedman:2000] to understand how a variable is related to the response. [Marginal dependence](#variabledependence) plots give us an idea of the overall trend of a variable/response relation, while [Partial dependence](#partialdependence) plots show us a risk adjusted relation. These figures often show strongly non-linear variable/response relations that are not easily obtained through a strictly parametric approach. 

Another key difference between a parametric model and random forests is that random forests are not parsimonious, instead using all variables in constructing a predictor. As such, we are also interested in examining [variable interactions](#interactions) within the forest model. Using a minimal depth approach, we can quantify how closely variables are related within the forest, and generate [marginal dependence](#coplots) and [partial dependence](#partialcoplots) (risk adjusted) conditioning plots (coplots)[@chambers:1992; @cleveland:1993] to examine the interactions graphically.

# Data: Boston Housing Values<a name="data"></a>

The Boston Housing data is a standard benchmark data set for regression models. It contains data for 506 census tracts of Boston from the 1970 census [@Harrison:1978; @Belsley:1980]. The data is available in multiple R packages, but to keep the dependencies for the `ggRandomForests` package down, we will use the data contained in the [MASS](http://CRAN.R-project.org/package=MASS) package, available with the base install of R. The following code block loads the data into the environment. 
The table details the Boston data set variable names, types and descriptions.
 
```{r datastep} 
# Load the Boston Housing data
data(Boston, package="MASS")

# Set modes correctly. For binary variables: transform to logical
Boston$chas <- as.logical(Boston$chas)
```

```{r cleanup, echo=FALSE, results="asis"}

cls <- sapply(Boston, class) 
# 
lbls <- 
  #crim
  c("Crime rate by town.",
    # zn
    "Proportion of residential land zoned for lots over 25,000 sq.ft.",
    # indus
    "Proportion of non-retail business acres per town.",
    # chas
    "Charles River (tract bounds river).",
    # nox
    "Nitrogen oxides concentration (10 ppm).",
    # rm
    "Number of rooms per dwelling.",
    # age
    "Proportion of units built prior to 1940.",
    # dis
    "Distances to Boston employment center.",
    # rad
    "Accessibility to highways.",
    # tax
    "Property-tax rate per $10,000.",
    # ptratio
    "Pupil-teacher ratio by town.",
    # black
    "Proportion of blacks by town.",
    # lstat
    "Lower status of the population (percent).",
    # medv
    "Median value of homes ($1000s).")

dta.labs <- data.frame(cbind(Variable=names(cls), Description=lbls, type=cls))

st.labs <- as.character(dta.labs$Description)
names(st.labs) <- names(cls)

print(xtable(dta.labs), type="html",include.rownames = FALSE)
```

## Exploratory Data Analysis<a name="eda"></a>

It is good practice to view your data before beginning an analysis, what @Tukey:1977 refers to as Exploratory Data Analysis (EDA). To facilitate this, we use the `ggplot` with the `ggplot2::facet_wrap` command to create two sets of panel plots, one for categorical variables with boxplots at each level, and one of scatter plots for continuous variables. Each variable is plotted along a selected continuous variable on the X-axis. These figures help to find outliers, missing values and other data anomalies in each variable before getting to deep into the analysis. We have created a separate [Shiny](shiny.rstudio.com) app, available at (https://ehrlinger.shinyapps.io/xportEDA), which creates similar figures for arbitrary data sets, to make the EDA process easier. 

The Boston housing data consists almost entirely of continuous variables, with the exception of the "Charles river" logical variable. A simple EDA visualization for this data is only a single panel plot of continuous variables, with observations colored by the single logical variable. Missing values in each variable are indicated by the rug marks along the x-axis, of which we have none. We used the Boston housing response variable, medv - the median value of homes, for X variable.

```{r data, fig.cap="EDA variable plots. Points indicate variable value against the median home value variable. Points are colored according to the chas variable.", fig.width=7, fig.height=5}
# Use tidyr to transform the data into long format.
dta <- Boston %>% gather(variable, value, -medv, -chas)

# plot panels for each covariate colored by the logical chas variable.
ggplot(dta, aes(x=medv, y=value, color=chas))+
  geom_point(alpha=.4)+
  geom_rug(data=dta %>% filter(is.na(value)))+
  labs(y="")+
  scale_color_brewer(palette="Set2")+
  facet_wrap(~variable, scales="free_y", ncol=3)
```

This figure is loosely related to a pairs scatter plot [@Becker:1988], but only examines the relation between one variable against the remainder. Plotting the data against the response variable also gives us a "sanity check" when viewing our model results. It's pretty obvious that we should find a strong relation between median home values and the `lstat` and `rm` variables.

# Random Forest - Regression<a name="rfr"></a>

A Random Forest is built up by bagging [@Breiman:1996] a collection of classification and regression trees [@cart:1984] (CART). The method uses a set of $B$ bootstrap [@bootstrap:1994] samples, growing an independent tree model on each sub-sample of the population. Each tree is grown by recursively partitioning the population based on optimization of a split rule over the $p$-dimensional covariate space. At each split, a subset of $m \le p$ candidate variables are tested for the split rule optimization, dividing each node into two daughter nodes. Each daughter node is then split until the process reaches the stopping criteria of either node purity or node member size, which defines the set of terminal (unsplit) nodes for the tree. In regression trees, node impurity is measured by mean squared error, whereas in classification problems, the Gini index is used [@Friedman:2000]. 

Random Forests sort each training set observation into one unique terminal node per tree. Tree estimates for each observation are constructed at each terminal node, among the terminal node members. The Random Forest estimate for each observation is then calculated by aggregating, averaging (regression) or votes (classification), the terminal node results across the collection of $B$ trees.

For this tutorial, we grow the random forest for regression using the `rfsrc` command to predict the median home value (`medv` variable) using the remaining 13 independent predictor variables. For this example we will use the default set of $B=1000$ trees (`ntree` argument), $m=5$ candidate variables (`mtry`) for each split with a stopping criteria of at most `nodesize=5` observations within each terminal node. 

Because growing random forests are computationally expensive, and the `ggRandomForests` package is targeted at the visualization of random forest objects, we will use cached copies of the `randomForestSRC` objects throughout this document. We include the cached objects as data sets in the `ggRandomForests` package. The actual `rfsrc` calls are included in comments within code blocks. 

```{r randomforest}
# Load the data, from the call:
# rfsrc_Boston <- rfsrc(medv~., data=Boston)
data(rfsrc_Boston)

# print the forest summary
rfsrc_Boston
```

The `randomForestSRC::print.rfsrc` summary details the parameters used for the `rfsrc` call, and returns variance and generalization error estimate from the forest training set.

One advantage of Random Forests is a built in generalization error estimate. Each bootstrap sample selects approximately 63.2% of the population on average. The remaining 36.8% of observations, the Out-of-Bag~ [@BreimanOOB:1996e] (OOB) sample, can be used as a hold out test set for each of the trees in the forest. An OOB prediction error estimate can be calculated for each observation by predicting the response over the set of trees which were NOT trained with that particular observation. Out-of-Bag prediction error estimates have been shown to be nearly identical to n--fold cross validation estimates [@StatisticalLearning:2009]. This feature of Random Forests allows us to obtain both model fit and validation in one pass of the algorithm.

The `gg_error` function operates on the `randomForestSRC::rfsrc` object to extract the error estimates as the forest is grown. The code block demonstrates part the `ggRandomForests` design philosophy, to create separate data objects and provide S3 functions to operate on the data objects. The following code block first creates a `gg_error` object, then uses the `plot.gg_error` function to create a `ggplot` object for display.

```{r error, fig.cap="Random forest generalization error. OOB error convergence along the number of trees in the forest."}
gg_e <- gg_error(rfsrc_Boston)
plot(gg_e)
```

This figure demonstrates that it does not take a large number of trees to stabilize the forest prediction error estimate. However, to ensure that each variable has enough of a chance to be included in the forest prediction process, we do want to run a rather large number of trees. 

The `gg_rfsrc` function extracts the OOB prediction estimates from the random forest. This code block executes the the data extraction and plotting in one line, since we are not interested in holding the prediction estimates for later reuse. Also note that we add in the additional `ggplot2` command (`coord_cartesian`) to modify the plot object. Each of the `ggRandomForests` S3 plot commands return `ggplot` objects, which we can also store for modification or reuse later in the analysis. 

```{r rfsrc, fig.cap="Random Forest OOB predicted median home values."}
plot(gg_rfsrc(rfsrc_Boston), alpha=.5)+
  coord_cartesian(ylim=c(5,49))
```

The `gg_rfsrc` plot shows the predicted median home value, one point for each observation in the training set. The estimates are OOB estimates, which are analogous to test set estimates. The boxplot is shown to give an indication of the distribution of the prediction estimates. For this analysis the figure is another model sanity check, as we are more interested in exploring the "why" of these predictions.

# Variable Selection<a name="varsel"></a>

Unlike parametric models, Random Forests do not require the explicit specification of the functional form of covariates to the response. Instead, RF ascertain which variables contribute to the prediction through the split rule optimization, optimally choosing variables which separate observations. We use two separate approaches to explore the RF selection process, [Variable Importance](#vimp) and [Minimal Depth](#minimaldepth).

## Variable Importance.<a name="vimp"></a>

_Variable importance_ (VIMP) was originally defined in CART using a measure involving surrogate variables (see Chapter 5 of [@cart:1984]). The most popular VIMP method uses a prediction error approach involving "noising-up" each variable in turn. VIMP for a variable `x_v` is the difference between prediction error when `x_v` is noised up by randomly permuting its values, compared to prediction error under the observed values [@Breiman:2001; @Liaw:2002; @Ishwaran:2007; @Ishwaran:2008].

Since VIMP is the absolute difference between OOB prediction error before and after permutation, a large VIMP value indicates that misspecification detracts from the variable predictive accuracy in the forest. VIMP close to zero indicates the variable contributes nothing to predictive accuracy, and negative values indicate the predictive accuracy _improves_ when the variable is mispecified. In the later case, we assume noise is more informative than the true variable. As such, we ignore variables with negative and near zero values of VIMP, relying on large positive values to indicate that the predictive power of the forest is dependent on those variables. 

The `gg_vimp` function extracts VIMP measures for each of the variables used to grow the forest. The `plot.gg_vimp` function shows the variables, in VIMP rank order, from the largest (Lower Status) at the top, to smallest (Charles River) at the bottom. VIMP measures is shown using bars to compare the scale of the error increase under permutation. 

For our random forest, the top two variables (Lower Status and number of rooms) display the largest VIMP, with a sizable difference to the remaining variables all showing similar VIMP measure. This indicates we should focus attention on these two variables, at least, over the others.

```{r vimp, fig.cap="Random forest VIMP plot.", fig.width=7, fig.height=5}
plot(gg_vimp(rfsrc_Boston), lbls=st.labs)
```

In this example, all VIMP measures are positive, though some are small. When there are both negative and positive VIMP values, the `plot.gg_vimp` function will color VIMP by the sign of the measure. We use the `lbls` argument to pass meaningful text descriptions to the figure, replacing the often terse variable names.

## Minimal Depth.<a name="minimaldepth"></a>

In VIMP, prognostic risk factors are determined by testing of the forest under alternative tests, ranking the most important variables according to impact on predictive ability of the forest. An alternative method uses inspection of the forest construction to rank variables. _Minimal depth_ assumes that variables with high impact on the prediction are those that most frequently split nodes nearest to the trunks of the trees (i.e. at the root node) where they partition large samples of the population. 

Node levels are numbered based on their relative distance to the trunk of the tree (with the root at 0). Minimal depth measures the important risk factors by averaging the depth of first split for each variable over all trees within the forest. Lower values of this measure indicate variables important in splitting large groups of patients. 

The _maximal subtree_ for a variable $x$ is the largest subtree whose root node splits on $x$. All parent nodes of $x$'s maximal subtree have nodes that split on variables other than $x$. The largest maximal subtree possible is at the root node. If a variable does not split the root node, it can have more than one maximal subtree. A maximal subtree may also not exist if there are no splits on the variable. The minimal depth of a maximal subtree (the first order depth) measures predictiveness of a variable $x$. The smaller the minimal depth, the more impact $x$ has on prediction. 

Given minimal depth is a quantitative property of the forest construction, @Ishwaran:2010 construct an analytic threshold for evidence of variable impact. A simple threshold rule uses the mean of the minimal depth distribution, classifying variables with minimal depth under the threshold as important in forest prediction. 

The `gg_minimal_depth` function is analogous to the `gg_vimp` function for minimal depth. Variables are ranked from most important at the top (minimal depth measure), to least at the bottom (maximal minimal depth). The vertical dashed line indicates the minimal depth threshold where smaller minimal depth values indicate higher importance and larger indicate lower importance.

The `randomForestSRC::var.select` call is again a computationally intensive function, as it traverses the forest finding the maximal subtree within each tree for each variable before averaging the results we use in the `gg_minimal_depth` call. We again use the cached object strategy here to save computational time. The `var.select` call is included in the comment of this code block.

```{r minimaldepth, fig.cap="Minimal Depth variables in rank order, most important at the top. Vertical dashed line indicates the maximal minimal depth for important variables.", fig.width=7, fig.height=5}
# Load the data, from the call:
# varsel_Boston <- var.select(rfsrc_Boston)
data(varsel_Boston)

# Save the gg_minimal_depth object for later use.
gg_md <- gg_minimal_depth(varsel_Boston)

# plot the object
plot(gg_md, lbls=st.labs)
```

The  minimal depth plot indicates ten variables which have a higher impact than the remaining three. In general, the selection of variables according to VIMP is to rather arbitrarily examine the values, looking for some point along the ranking where there is a large difference in VIMP measures. The minimal depth threshold method has a more quantitative approach to determine a selection threshold.

Since the VIMP and Minimal Depth measures use different criteria, we expect the variable ranking to be slightly different. We use `gg_minimal_vimp` function to compare rankings between minimal depth and VIMP. The points along the red dashed line indicate the measures are in agreement. In this call, we plot the stored `gg_minimal_depth` object (`gg_md`), which would be equivalent to calling `plot.gg_minimal_vimp(varsel_Boston)` or `plot(gg_minimal_vimp(varsel_Boston))`.

```{r minimalvimp, fig.cap="Comparing Minimal Depth and Vimp rankings. Points on the red dashed line are ranked equivalently, points below have higher VIMP, those above have higher minimal depth ranking. Variables are colored by the sign of the VIMP measure"}
plot.gg_minimal_vimp(gg_md)
```

Points above the red dashed line are ranked higher by VIMP than by minimal depth, indicating the variables are sensitive to misspecification. Those below the line have a higher minimal depth ranking, indicating they are better at dividing large portions of the population. The further the points are from the line, the more the discrepancy between measures. The construction of this figure is skewed towards a minimal depth approach, by ranking variables along the y-axis, though points are colored by the sign of VIMP. 

In our example, both minimal depth and VIMP indicate the strong relation of `lstat` and `rm` variables to the forest prediction, which agrees with our expectation from the [EDA](#eda) done at the beginning of this document.

# Response Dependency.<a name="dependence"></a>

As random forests are not a parsimonious methodology, we use the minimal depth and VIMP measures to reduce the number of variables we need to examine to a manageable subset. Once we have an idea of which variables contribute most to the predictive accuracy of the forest, we would like to know how the response depends on these variables.

Although often characterized as a "black box" method, it is possible to express a random forest in functional form. In the end the forest predictor is some function, although complex, of the predictor variables $\hat{f}_{rf} = f(x)$. We use graphical methods to examine the forest predicted response dependency on covariates. We again look at two options, [variable dependence](#variabledependence) plots and [partial dependence](#partialdependence) plots. 

## Variable Dependence<a name="variabledependence"></a>

Variable dependence plots show the predicted response as a function of a covariate of interest, where each observation is represented by a point on the plot. Each predicted point is an individual observations, dependent on the full combination of all other covariates, not only on the covariate of interest. Interpretation of variable dependence plots can only be in general terms, as point predictions are a function of all covariates in the model. However, variable dependence is straight forward to calculate, involving only the predicted response for each observation.

We use the `gg_variable` function call to extract the variables and the predicted OOB response from `randomForestSRC::rfsrc` and `randomForestSRC::predict` objects. In the following code block, we will store the `gg_variable` data object for later use as all variable dependence plots can be constructed from this (`gg_v`) object. Then get the minimal depth selected variables (minimal depth lower than the threshold value) from the previously stored `gg_minimal_depth` object (`gg_md$topvars`).

The `plot.gg_variable` function call operates in the `gg_variable` object. We pass it the list of variables of interest (`xvar`) and request a single panel (`panel=TRUE`) to display the figures. By default, the `plot.gg_variable` function returns a list of `ggplot` objects, one for each variable named in `xvar` argument. The next three arguments are passed to internal `ggplot` plotting routines. The `se` and `span` arguments are used to modify the internal call to `ggplot2::geom_smooth` for fitting smooth lines to the data. The `alpha` argument lightens the coloring points in the `ggplot2::geom_point` call, making it easier to see point over plotting. We also demonstrate modifying the plot labels using the `ggplot2::labs` function.

```{r variable, fig.cap="Variable dependence plot. Individual case predictions are marked with points. Loess smooth curve indicates the trend as the variables increase with shaded 95% confidence band.", fig.width=7, fig.height=5}
# Create the variable dependence object from the random forest
gg_v <- gg_variable(rfsrc_Boston)

# We want the top minimal depth variables only,
# plotted in minimal depth rank order. 
xvar <- gg_md$topvars

# plot the variable list in a single panel plot
plot(gg_v, xvar=xvar, panel=TRUE, 
     se=.95, span=1.2, alpha=.4)+
  labs(y=st.labs["medv"], x="")
```

This figure will look very similar to the [EDA](#eda) figure, although with transposed the axis as we plot the response variable on the y-axis. The closer the panels match, the better the RF prediction. We've included a smooth loess line [@cleveland:1981; @cleveland:1988] to indicates the trend of the prediction dependence over the covariate values.

The Boston housing data does contain a single categorical variable, the Charles river logical variable. Variable dependence plots for categorical variables are constructed using boxplots to show the distribution of the predictions within each category. Although the Charles river variable has the lowest importance scores in both VIMP and minimal depth measures, we include the variable dependence plot as an example of categorical variable dependence.

```{r chas, fig.cap="Variable dependence for Charles River logical variable."}
plot(gg_v, xvar="chas", points=FALSE,
     se=FALSE, notch=TRUE, alpha=.4)+
  labs(y=st.labs["medv"])+
  coord_cartesian(ylim=c(5,49))
```

The figure shows that most housing tracts do not border the Charles river (`chas=FALSE`), and comparing the distributions of the predicted median housing values indicates no significant difference in home values. This reinforces the findings in both VIMP and Minimal depth, the Charles river variable has very little impact on the forest prediction.  

There is not a convenient method to panel scatter plots and boxplots together, so we recommend creating panel plots for each variable type separately. 

## Partial Dependence.<a name="partialdependence"></a>

Partial variable dependence plots are a risk adjusted alternative to variable dependence. Partial plots are generated by integrating out the effects of all variables beside the covariate of interest. Partial dependence data are constructed by selecting points evenly spaced along the distribution of the $X$ variable of interest. For each value ($X = x$), we calculate the average RF prediction over all other covariates in $X$ by
$$ \tilde{f}(x) = \frac{1}{n} \sum_{i = 1}^n \hat{f}(x, x_{i, o}), $$
where $\hat{f}$ is the predicted response from the random forest and $x_{i, o}$ is the value for all other covariates other than $X = x$ for the observation $i$ [@Friedman:2000]. Essentially, we average a set of nomograms for each observation in the training set at the $X=x$. Repeating the process for a sequence of $X=x$ values to generate the partial plot. 

Partial plots are another computationally intensive analysis, especially when there are a large number of observations. We again turn to our data cache strategy here. The default parameters for the `randomForestSRC::plot.variable` function generate partial dependence estimates at `npts=25` points. For each point of interest, the `plot.variable` function averages `n` response predictions. This is repeated for each of the variables of interest. 

```{r partial, fig.cap="Partial dependence panels.", fig.width=7, fig.height=5}
# Load the data, from the call:
# partial_Boston <- plot.variable(rfsrc_Boston, 
#                                 xvar=gg_md$topvars, 
#                                 partial=TRUE, sorted=FALSE, 
#                                 show.plots = FALSE )
data(partial_Boston)

# generate a list of gg_partial objects, one per xvar.
gg_p <- gg_partial(partial_Boston)

# plot the variable list in a single panel plot
plot(gg_p, xvar=xvar, panel=TRUE, se=FALSE) +
  labs(y=st.labs["medv"], x="")
```

We again order the panels by minimal depth ranking. We see again how the `lstat` and `rm` variables are strongly related to the median value response, making the partial dependence of the remaining variables look flat. We also see strong nonlinearity of these two variables. The `lstat` variable looks rather quadratic, while the `rm` shape is more complex.  

# Variable Interactions<a name="interactions"></a>

Using the different variable dependence measures, it is also possible to calculate measures of pairwise interactions among variables. Recall that minimal depth measure is defined by averaging the tree depth of variable $i$ relative to the root node. For interactions, the calculation can be modified to be the minimal depth of a variable $j$ with respect to the maximal subtree for variable $i$ [@Ishwaran:2010; @Ishwaran:2011].

The `randomForestSRC::find.interaction` traverses the forest, calculating all pairwise minimal depth interactions, and returns a $p \times p$ matrix of interaction measures. The diagonal terms are normalized to the root node, and off diagonal terms are normalized measures of pairwise variable interaction. 

The `gg_interaction` function wraps the `find.interaction` matrix for the S3 plot functions. The `xvar` argument indicates which variables we're interested in. We again use the `panel=TRUE` option.

```{r interactions, fig.cap="Minimal depth variable interactions. Reference variables are marked with red cross in each panel. Higher values indicate lower interactivity with reference variable.", fig.width=7, fig.height=5}
# Load the data, from the call:
# interaction_Boston <- find.interactions(rfsrc_Boston)
data(interaction_Boston)

# Plot the results in a single panel.
plot(gg_interaction(interaction_Boston), 
     xvar=gg_md$topvars, panel=TRUE)
```

The `gg_interaction` figure plots the interactions for the target variable (shown in the red cross) with interaction scores for all remaining variables. We expect the covariate with lowest minimal depth (`lstat`) to be associated with almost all other variables, as it typically splits close to the root node, so viewed alone may not be as informative as looking at a collection of interactive depth plots. Scanning across the panels, we see each successive target depth increasing as expected. We also see the interactive variables increasing along the with increasing target depth. Of interest here, is the interaction of `lstat` with the `rm` variable shown in the `rm` panel. Aside from these being the strongest variables by both measures, this interactive measure indicates the strongest connection between variables. We explore this further in the following sections. 

# Coplots<a name="coplots"></a>

Conditioning plots (coplots) [@chambers:1992; @cleveland:1993]  are a powerful visualization tool to efficiently study how a response depends on two or more variables[@cleveland:1993]. The method allow us to view data by grouping observations on some conditional membership. The simplest example involves a categorical variable, where we plot our data conditional on class membership, for instance on the Charles river logical variable. We can view a coplot as a stratified variable dependence plot, indicating trends in the RF prediction results within panels of group membership.

Interactions with a continuous variable requires stratification at some level. Often we can make these stratification along some feature of the variable, for instance a variable with integer values, or 5 or 10 year age group cohorts. However in the variable of interest in our Boston housing example, we have no "logical" stratification indications. We will arbitrarily stratify our variables into  6 groups of roughly equal population size using the `cut_distribution` function. We pass the break points to the `cut` function to create intervals, which we can add to the `gg_variable` object before plotting. The key to coplots is to use the `ggplot2::facet_wrap` command to generate a panel for each interval.

We start by examining the predicted median home value as a function of lower status percentage (`lstat`) conditional on membership within 6 groups of average number of rooms (`rm`) "intervals". 
```{r coplots, fig.cap="Variable Coplots. Predicted median home values as a function of percentage of lower status population, stratified by average number of rooms groups.", fig.width=7, fig.height=5}
# Find the rm variable points to create 6 intervals of roughly 
# equal size population
rm_pts <- cut_distribution(rfsrc_Boston$xvar$rm, groups=6)

# Pass these variable points to create the 6 (factor) intervals
rm_grp <- cut(rfsrc_Boston$xvar$rm, breaks=rm_pts)

# Append the group factor to the gg_variable object
gg_v$rm_grp <- rm_grp

# Modify the labels for descriptive panel titles 
levels(gg_v$rm_grp) <- paste("Rooms=", levels(gg_v$rm_grp), sep="")

# Create a variable dependence (co)plot, faceted on group membership.
var_dep <- plot(gg_v, xvar = "lstat", smooth = TRUE, 
                method = "loess", span=1.5, alpha = .5, se = FALSE) + 
  labs(y = st.labs["medv"], x=st.labs["lstat"]) + 
  theme(legend.position = "none") + 
  scale_color_brewer(palette = "Set3") + 
  #scale_shape_manual(values = event.marks, labels = event.labels)+ 
  facet_wrap(~rm_grp)

var_dep
```

The figure shows the median value response as a function of lower status percentage, grouped conditional on intervals of the average number of rooms within the observation. We again use the smooth loess curve to get an idea of the trend within each group. Overall, median values decrease with increasing percentage of lower status population, and increases with increasing average number of rooms. In addition to trends, we can also examine the conditional distribution of variables. Note that smaller homes in higher status neighborhoods still have high median values, and that there are more large homes in the higher status neighborhoods. 

A single coplot gives us a grouped view of a variable, along the second variable dimension. To get a better feel for how the response depends on both variables, it is instructive to look at the complementary coplot. We repeat the previous coplot, predicted median home value as a function of the average number of rooms (`rm`) variable, conditional on membership within 6 groups of lower status percentage (`lstat`) intervals. 

```{r coplots2, fig.cap="Variable Coplots. Predicted median home value as a function of average number of rooms, stratified by percentage of lower status groups.", fig.width=7, fig.height=5}
# Find the lstat variable points to create 6 intervals of roughly 
# equal size population
lstat_pts <- cut_distribution(rfsrc_Boston$xvar$lstat, groups=6)

# Pass these variable points to create the 6 (factor) intervals
lstat_grp <- cut(rfsrc_Boston$xvar$lstat, breaks=lstat_pts)

# Append the group factor to the gg_variable object
gg_v$lstat_grp <- lstat_grp

# Modify the labels for descriptive panel titles 
levels(gg_v$lstat_grp) <- paste("Low Stat=", levels(gg_v$lstat_grp),"%",sep="")

# Create a variable dependence (co)plot, faceted on group membership.
var_dep <- plot(gg_v, xvar = "rm", smooth = TRUE, 
                method = "loess", span=1.5, alpha = .5, se = FALSE) + 
  labs(y = st.labs["medv"], x=st.labs["rm"]) + 
  theme(legend.position = "none") + 
  scale_color_brewer(palette = "Set3") + 
  #scale_shape_manual(values = event.marks, labels = event.labels)+ 
  facet_wrap(~lstat_grp)

var_dep
```

We get similar information from this view, home values decrease with increasing percentage of lower status and decreasing number of rooms. However views together we get a better sense of how the lower status and rooms variables work together (interact) in the median value prediction.

Note that typically, @cleveland:1993 conditional plots for continuous variables included overlapping intervals along the variable. We chose to use mutually exclusive continuous variable intervals for multiple reasons:
 
 * Simplicity - We can create the coplot figures directly from the `gg_variable` object by adding a conditional group column directly to the object.

 * Interpretability - We find it easier to interpret and compare the panels if each observation is only in a single panel.

 * Clarity - We prefer using more space for the data portion of the figures than typically displayed in the `coplot` function available in base R.
 
It is still possible to augment the `gg_variable` to include overlapping conditional membership with continuous variables by duplicating rows of the object, and setting the correct conditional group membership. The `plot.gg_variable` function recipe above could be used to generate the panel plot, with panels ordered according to the factor levels of the grouping variable.  We leave this as an exercise for the reader.

## Partial dependence coplots <a name="partialcoplots"></a>

By characterizing conditional plots as stratified variable dependence plots, the next logical step would be to generate an analogous partial dependence conditional plot. The process is similar to variable dependence coplots, first determine conditional group membership, then calculate the partial dependence estimates on each subgroup using the `randomForestSRC::plot.variable` function using the `subset` argument on each grouping. 

The `ggRandomForests::gg_partial_coplot` function is a wrapper for generating conditional partial dependence data object. Given a `randomForestSRC::rfsrc` and a `groups` vector for conditioning the training data set observations, `gg_partial_coplot` calls the `randomForestSRC::plot.variable` function for each set of training set observations conditional on `groups`. The function returns a `gg_partial_coplot` object, a sub class of the `gg_partial` object, which can be plotted with the `plot.gg_partial` function.

The following code block will generate the data object for creating partial dependence coplot of the predicted median home value as a function of lower status percentage (`lstat`) conditional on membership within the 6 groups of average number of rooms (`rm`) "intervals" we examined in the previous section.

```{r prtl-copl, eval=FALSE}
partial_coplot_Boston <- gg_partial_coplot(rfsrc_Boston, xvar="lstat", 
                                           groups=rm_grp,
                                           show.plots=FALSE)
```

Since the `gg_partial_coplot` makes a call to `randomForestSRC::plot.variable` for each group (6) in the conditioning set, we again resort to the data caching strategy, and load the stored result data from the `ggRandomForests` package. 

```{r prtl-coplots, fig.cap="Partial Coplots. Risk adjusted predicted median value as a function of Lower Status, conditional on groups of average number of rooms.", fig.width=7, fig.height=5}
# Load the stored partial coplot data.
data(partial_coplot_Boston)

# Partial coplot
ggpl <- plot(partial_coplot_Boston, se=FALSE)+
  labs(x=st.labs["lstat"], y=st.labs["medv"], 
       color="Room", shape="Room")+
  scale_color_brewer(palette="Set1")
ggpl
```

Unlike variable dependence coplots, we do not need to use the panel format for partial dependence coplots because we are looking risk adjusted estimates (points) instead of population estimates. The figure shows a loess curve through the point estimates conditional on the room interval grouping. The figure again indicates that larger homes (average room size from 6.87 and up, shown in yellow) have a higher median value then the others. In neighborhoods with higher percentage of low status population, the Median values decrease with number of rooms until it stabilizes from the intervals between 5.73 and 6.47, then decreases again for homes smaller than 5.73. In higher status neighborhoods, the effect of smaller homes is not noticeable.

We can view the partial coplot curves as slices along a surface going into the page, either along increasing or decreasing average room size. This is made more difficult by our choice to select groups of similar population size, as the curves are not evenly spaced along the `rm` variable. We return to this problem in the next section. 

Turning back to the alternative view, for partial dependence coplot of the predicted median home value as a function of average number of rooms (`rm`) conditional on membership within the 6 groups of lower status percentage (`lstat`)  "intervals", we will cache the following `gg_partial_coplot` data call.

```{r prtl-copl2, eval=FALSE}
partial_coplot_Boston2 <- gg_partial_coplot(rfsrc_Boston, xvar="rm", 
                                            groups=lstat_grp,
                                            show.plots=FALSE)
```

The `plot.gg_variable` call:
```{r prtl-coplots2, fig.cap="Partial Coplots. Risk adjusted predicted median value as a function of average number of rooms, conditional on groups of percentage of lower status population.", fig.width=7, fig.height=5}
# Load the stored partial coplot data.
data(partial_coplot_Boston2)

# Partial coplot
ggpl <- plot(partial_coplot_Boston2, se=FALSE)+
  labs(x=st.labs["rm"], y=st.labs["medv"], 
       color="Lower Status", shape="Lower Status")+
  scale_color_brewer(palette="Set1")
ggpl
```

This figure indicates that the median home value does not change much until the average size increases above 6.5 rooms, then flattens again above 8 rooms, regardless of the status of the neighborhood. This agrees well with the `rm` partial plot shown earlier. Care must be taken in interpreting the even spacing of the curves along the percentage of lower status groupings, as again, we chose these groups to have similar sized populations, not to be evenly spaced along the `lstat` variable. 

## Partial coplot surfaces<a name="coplotsurface"></a>

Visualizing two dimensional projections of three dimensional data is difficult, though there are tools available to make the data more understandable. To make the interplay of lower status and average room size a bit more understandable, we will generate a contour plot of the median home values. We could generate this figure with the data we already have, but the resolution would be a bit strange. To generate the `lstat` conditional on '`rm` groupings, we would end up with contours over a grid of $25 \times 6$, for the alternative `rm` conditional on `lstat` groups, we'd have the transpose grid of $6 \times 25$. 

Since we are already using the data caching strategy, we will generate another `gg_partial_coplot` data set with increased resolution in both the `lstat` and `rm` dimensions. For this exercise, we will create 50 `rm` groups and generate the partial plot data at `npts=50` points in the `plot.variable` call. First generate the 50 `rm` groups, each containing around 9 observations.     

```{r def-pts}
# Find the quantile points to create 50 interval groups
rm_pts <- cut_distribution(rfsrc_Boston$xvar$rm, groups=50)

# generate the grouping intervals.
rm_grp <- cut(rfsrc_Boston$xvar$rm, breaks=rm_pts)
```

Then the data call to generate the `gg_partial_coplot` data object. This took about 15 minutes to run on a quad core Mac Air.
```{r prtl-surface, eval=FALSE}
# Generate the gg_partial_coplot data object
system.time(partial_coplot_Boston_surf <- gg_partial_coplot(rfsrc_Boston, xvar="lstat", 
                                                 groups=rm_grp, npts=50,
                                                 show.plots=FALSE))

# user  system elapsed 
# 848.266  79.705 934.584 
```

The `gg_partial_coplot` data object is included as a data set in the `ggRandomForests` package. We load the data, attach numeric values for the `rm` groups, and generate the figure.

```{r contour3d, fig.cap="Partial coplot surface.", fig.width=7, fig.height=5}
# Load the stored partial coplot data.
data(partial_coplot_Boston_surf)

# Instead of groups, we want the raw rm point values,
# To make the dimensions match, we need to repeat the values
# for each of the 50 points in the lstat direction
rm.tmp <- do.call(c,lapply(rm_pts[-1], 
                 function(grp){rep(grp, 50)}))

# attach the data to the gg_partial_coplot
partial_coplot_Boston_surf$rm <- rm.tmp

# ggplot2 contour plot of x, y and z data.
ggplot(partial_coplot_Boston_surf, aes(x=lstat, y=rm, z=yhat))+
  stat_contour(aes(colour = ..level..), binwidth = 1)+
  labs(x=st.labs["lstat"], y=st.labs["rm"], 
       color="Median Home Values")+
  scale_colour_gradientn(colours=topo.colors(10))
```

This figure reinforces the previous findings, where lower home values are associated with higher percentage of lower status population, and higher values are associated with larger homes. The difference in this figure is we see how the values change as we move around the map of `lstat` and `rm` combinations. 

# Conclusion<a name="conclusion"></a>

In this vignette, we have demonstrated using the [ggRandomForests](http://CRAN.R-project.org/package=ggRandomForests)  to explore a regression random forest built with the [randomForestSRC](http://CRAN.R-project.org/package=randomForestSRC) package. Have covered [creating a random forest model](#rfsrc) and determining which variables contribute to the forest prediction accuracy using both [VIMP](#vimp) and [Minimal Depth](#minimaldepth) measures. We outlined how to investigate variable associations with the response variable using [variable dependence](#variabledependence) and the risk adjusted [partial dependence](#partialdependence) plots. We've also explored variable interactions by exploring the forest construction with pairwise [minimal depth interactions](#interactions) and directly using [variable dependence coplots](#coplots) and [partial dependence coplots](#partialcoplots).

Along the way, we've demonstrated the use of additional commands from the [ggplot2](http://CRAN.R-project.org/package=ggplot2) package for modifying and customizing results from [ggRandomForests](http://CRAN.R-project.org/package=ggRandomForests). 




# References<a name="references"></a>
